diff --git a/train_PPO_trXL.py b/train_PPO_trXL.py
index 460403f5..598f21d1 100644
--- a/train_PPO_trXL.py
+++ b/train_PPO_trXL.py
@@ -18,7 +18,7 @@ config = {
     "LR": 2e-4,
     "NUM_ENVS": 512,
     "NUM_STEPS": 128,
-    "TOTAL_TIMESTEPS": 1e7,
+    "TOTAL_TIMESTEPS": 1e8,
     "UPDATE_EPOCHS": 4,
     "NUM_MINIBATCHES": 8,
     "GAMMA": 0.99,
@@ -44,6 +44,9 @@ config = {
     "WANDB_PROJECT": "lit-transformer-ppo",
     "WANDB_ENTITY": "maharishiva",  # Set to your wandb username or team name
     "WANDB_LOG_FREQ": 1,    # Log every N updates
+    "THINKING_VOCAB": 65,
+    "R_THINK": -0.005,
+    "MAX_THINKING_LEN": 8,
 }
 
 # Initialize wandb if enabled
diff --git a/trainer_PPO_trXL.py b/trainer_PPO_trXL.py
index 5b9dcad4..42d07f9f 100644
--- a/trainer_PPO_trXL.py
+++ b/trainer_PPO_trXL.py
@@ -22,14 +22,17 @@ from wrappers import (
     OptimisticResetVecEnvWrapper,
     AutoResetEnvWrapper,
     BatchEnvWrapper,
+    ThinkingWrapper,
 )
 
 
 from transformerXL import Transformer
 
 class ActorCriticTransformer(nn.Module):
-    action_dim: Sequence[int]
-    activation: str 
+    # action_dim: int
+    action_dim_env: int
+    thinking_vocab: int
+    activation: str
     hidden_layers:int
     encoder_size: int
     num_heads: int
@@ -40,6 +43,7 @@ class ActorCriticTransformer(nn.Module):
     
     
     def setup(self):
+        self.action_dim = self.action_dim_env + self.thinking_vocab
         
         # USE SETUP AND DIFFERENT FUNCTIONS BECAUSE THE TRAIN IS DIFFERENT FROM EVAL ( as we query just one step in train and don't cache memory in eval)
         
@@ -55,7 +59,8 @@ class ActorCriticTransformer(nn.Module):
                                 num_layers=self.num_layers,
                                 gating=self.gating,
                                 gating_bias=self.gating_bias,
-                                action_dim=self.action_dim)
+                                env_action_dim=self.action_dim_env,
+                                thinking_vocab=self.thinking_vocab)
         
         self.actor_ln1=nn.Dense(self.hidden_layers, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))
         self.actor_ln2= nn.Dense(
@@ -154,7 +159,7 @@ class Transition(NamedTuple):
     prev_action: jnp.ndarray = None
     prev_reward: jnp.ndarray = None
 
-    
+
     
 indices_select=lambda x,y:x[y]
 batch_indices_select=jax.vmap(indices_select)
@@ -176,7 +181,9 @@ def make_train(config):
         from craftax.craftax.envs.craftax_symbolic_env import CraftaxSymbolicEnvNoAutoReset
         env=CraftaxSymbolicEnvNoAutoReset()
         env_params=env.default_params
+        action_dim_env = env.action_space(env_params).n
         env = LogWrapper(env)
+        env = ThinkingWrapper(env, action_dim_env, config["R_THINK"])
         env = OptimisticResetVecEnvWrapper(
                 env,
                 num_envs=config["NUM_ENVS"],
@@ -184,8 +191,10 @@ def make_train(config):
             )
     else:
         env, env_params = gymnax.make(config["ENV_NAME"])
+        action_dim_env = env.action_space(env_params).n 
         env = FlattenObservationWrapper(env)
         env = LogWrapper(env)
+        env = ThinkingWrapper(env, action_dim_env, config["R_THINK"])
         env = BatchEnvWrapper(env,config["NUM_ENVS"])
 
     def linear_schedule(count):
@@ -196,7 +205,8 @@ def make_train(config):
     def train(rng):
 
         # INIT NETWORK
-        network=ActorCriticTransformer(action_dim=env.action_space(env_params).n,
+        network=ActorCriticTransformer(action_dim_env=env.action_space(env_params).n,
+                             thinking_vocab=config["THINKING_VOCAB"],
                              activation=config["ACTIVATION"],
                             encoder_size=config["EMBED_SIZE"],
                             hidden_layers=config["hidden_layers"],
@@ -329,15 +339,20 @@ def make_train(config):
             def _calculate_gae(traj_batch, last_val):
                 def _get_advantages(gae_and_next_value, transition):
                     gae, next_value = gae_and_next_value
-                    done, value, reward = (
+                    done, value, reward, action = (
                         transition.done,
                         transition.value,
                         transition.reward,
+                        transition.action
                     )
-                    delta = reward + config["GAMMA"] * next_value * (1 - done) - value
+                    is_thinking = jnp.greater_equal(action, network.action_dim_env)
+                    # apply thinking penalty to the reward
+                    # reward = reward + jnp.where(is_thinking, config["R_THINK"], 0.0)
+                    gamma = jnp.where(is_thinking, 1.0, config["GAMMA"])
+                    delta = reward + gamma * next_value * (1 - done) - value
                     gae = (
                         delta
-                        + config["GAMMA"] * config["GAE_LAMBDA"] * (1 - done) * gae
+                        + gamma * config["GAE_LAMBDA"] * (1 - done) * gae
                     )
                     return (gae, value), gae
 
@@ -499,6 +514,7 @@ def make_train(config):
                 "update": runner_state[0].step_count,
                 "return": metric["returned_episode_returns"],
                 "episode_length": metric["returned_episode_lengths"],
+                "thinking_count": metric["thinking_count"],
             }
             
             # Run the update epochs
@@ -514,18 +530,19 @@ def make_train(config):
             
             # Log to wandb if enabled
             if config.get("WANDB_MODE", "disabled") == "online":
-                def callback(update, return_val, episode_length, timesteps):
+                def callback(update, return_val, episode_length, timesteps, thinking_count):
                     # Log every WANDB_LOG_FREQ updates
                     if update % config["WANDB_LOG_FREQ"] == 0:
                         wandb.log({
                             "return": float(return_val),
                             "episode_length": float(episode_length),
                             "timesteps": int(timesteps),
-                            "update": int(update)
+                            "update": int(update),
+                            "thinking_count": float(thinking_count),
                         })
                 
                 jax.debug.callback(callback, metrics["update"], metrics["return"], 
-                                  metrics["episode_length"], metrics["timesteps"])
+                                  metrics["episode_length"], metrics["timesteps"], metrics["thinking_count"])
             
             rng = update_state[-1]
             # Reset step_env_currentloop to 0, but keep last_action and last_reward for the next batch
diff --git a/transformerXL.py b/transformerXL.py
index cfef0fd2..77c29ebd 100644
--- a/transformerXL.py
+++ b/transformerXL.py
@@ -112,19 +112,23 @@ class Transformer(nn.Module):
     num_heads: int
     qkv_features: int
     num_layers: int
+    env_action_dim: int  # number of real environment actions
+    thinking_vocab: int  # number of thinking actions
     gating: bool = False
     gating_bias: float = 0.
-    action_dim: int = None
+
+    @property
+    def total_action_dim(self):
+        return self.env_action_dim + self.thinking_vocab
 
     def setup(self):
         self.encoder = nn.Dense(self.encoder_size)
         
-        # Add embeddings for previous action and reward
-        if self.action_dim is not None:
-            self.action_embed = nn.Embed(num_embeddings=self.action_dim, features=self.encoder_size)
-            self.reward_embed = nn.Dense(self.encoder_size)
-            # Projection layer for combined embeddings
-            self.token_projection = nn.Dense(self.encoder_size)
+        # separate embeddings for environment and thinking actions
+        self.env_action_embed = nn.Embed(num_embeddings=self.env_action_dim, features=self.encoder_size)
+        self.thinking_embed = nn.Embed(num_embeddings=self.thinking_vocab, features=self.encoder_size)
+        self.reward_embed = nn.Dense(self.encoder_size)
+        self.token_projection = nn.Dense(self.encoder_size)
         
         self.tf_layers = [transformer_layer(num_heads=self.num_heads, qkv_features=self.qkv_features,
                                            out_features=self.encoder_size,
@@ -132,33 +136,34 @@ class Transformer(nn.Module):
         
         self.pos_emb = PositionalEmbedding(self.encoder_size)
 
-    def _create_combined_token(self, obs, prev_action=None, prev_reward=None):
-        # Encode observation
+    def _create_combined_token(self, obs, prev_action, prev_reward):
+        """Create a token from observation and (prev_action, prev_reward).
+
+        If prev_action corresponds to a thinking action, the token is *only* the
+        thinking-action embedding.  Otherwise the token is the projected
+        concatenation of [state_emb, action_emb, reward_emb].
+        """
         state_emb = self.encoder(obs)
-        
-        # If action_dim is specified and prev_action/prev_reward are provided, create combined token
-        if self.action_dim is not None and prev_action is not None and prev_reward is not None:
-            # Embed previous action and reward
-            action_emb = self.action_embed(prev_action)
-            
-            # Apply reward embedding to prev_reward
-            # For scalar reward values, add a new axis
-            if state_emb.ndim > prev_reward.ndim:
-                # Add feature dimension for the reward
-                reward_emb = self.reward_embed(prev_reward[..., None])
-            else:
-                # If reward already has enough dimensions
-                reward_emb = self.reward_embed(prev_reward)
-            
-            # Concatenate embeddings - ensure all have same dimensions
-            combined = jnp.concatenate([state_emb, action_emb, reward_emb], axis=-1)
-            
-            # Project back to encoder size
-            token = self.token_projection(combined)
-            return token
-        else:
-            # Fall back to just observation encoding if prev_action/prev_reward not provided
-            return state_emb
+
+        # Determine whether the previous action was a thinking action.
+        is_think = jnp.greater_equal(prev_action, self.env_action_dim)  # bool array
+
+        # Compute indices for embedding look-ups (keep them in-range regardless of branch)
+        env_idx = jnp.where(is_think, 0, prev_action)
+        think_idx = jnp.where(is_think, prev_action - self.env_action_dim, 0)
+
+        env_action_emb = self.env_action_embed(env_idx)
+        thinking_emb = self.thinking_embed(think_idx)
+
+        # Embed reward (always used in non-thinking branch)
+        reward_emb = self.reward_embed(prev_reward[..., None] if state_emb.ndim > prev_reward.ndim else prev_reward)
+
+        # Build env token: concat then linear projection
+        env_token = self.token_projection(jnp.concatenate([state_emb, env_action_emb, reward_emb], axis=-1))
+
+        # Select between thinking token and env token
+        token = jnp.where(is_think[..., None], thinking_emb, env_token)
+        return token
     
     def __call__(self, memories, obs, mask, prev_action=None, prev_reward=None):
         # Create token from obs, prev_action, and prev_reward
diff --git a/wrappers.py b/wrappers.py
index f2d38416..d290fd43 100644
--- a/wrappers.py
+++ b/wrappers.py
@@ -149,6 +149,72 @@ class OptimisticResetVecEnvWrapper(GymnaxWrapper):
         return obs, state, reward, done, info
 
 
+@struct.dataclass
+class ThinkingEnvState:
+    env_state: Any
+    thinking_count: int
+    last_obs: Any
+    last_info: Any
+
+class ThinkingWrapper(GymnaxWrapper):
+    def __init__(self, env, env_action_dim: int, think_reward: float = -0.005):
+        super().__init__(env)
+        self.env_action_dim = int(env_action_dim)
+        self.think_reward = float(think_reward)
+
+    @partial(jax.jit, static_argnums=(0, 2))
+    def reset(self, key, params=None):
+        obs, inner_state = self._env.reset(key, params)
+
+        info_template = {
+            "returned_episode_returns": jnp.array(0.0,  jnp.float32),
+            "returned_episode_lengths": jnp.array(0,    jnp.int32),
+            "timestep":                jnp.array(0,    jnp.int32),
+            "returned_episode":        jnp.array(False),
+            "thinking_action":         jnp.array(False),
+            "thinking_count":          jnp.array(0,    jnp.int32),
+        }
+        state = ThinkingEnvState(inner_state, 0, obs, info_template)
+        return obs, state
+
+    @partial(jax.jit, static_argnums=(0, 4))
+    def step(self, key, state: ThinkingEnvState, action, params=None):
+        is_think = action >= self.env_action_dim
+        operand = (key, state, action, params)
+        def _think(op):
+            key, st, act, prm = op
+            info = dict(st.last_info) 
+            info["thinking_action"] = True
+            info["thinking_count"]  = st.thinking_count + 1
+            reward = jnp.asarray(self.think_reward, jnp.float32)
+            done   = jnp.array(False)
+            new_state = ThinkingEnvState(
+                st.env_state, st.thinking_count + 1, st.last_obs, info
+            )
+            return st.last_obs, new_state, reward, done, info
+
+
+        def _act(op):
+            key, st, act, prm = op
+            obs, inner_state, reward, done, info_inner = self._env.step(
+                key, st.env_state, act, prm
+            )
+
+            info = dict(st.last_info)
+            info["returned_episode_returns"]  = info_inner["returned_episode_returns"]
+            info["returned_episode_lengths"]  = info_inner["returned_episode_lengths"]
+            info["returned_episode"]          = info_inner["returned_episode"]
+            info["timestep"]                  = info_inner["timestep"]
+            info["thinking_action"]           = False
+            info["thinking_count"]            = st.thinking_count
+
+            new_state = ThinkingEnvState(
+                inner_state, st.thinking_count, obs, info
+            )
+            return obs, new_state, reward, done, info
+
+        return jax.lax.cond(is_think, _think, _act, operand)
+    
 @struct.dataclass
 class LogEnvState:
     env_state: Any
@@ -165,13 +231,13 @@ class LogWrapper(GymnaxWrapper):
     def __init__(self, env):
         super().__init__(env)
 
-    @partial(jax.jit, static_argnums=(0, 2))
+    @partial(jax.jit, static_argnums=0)
     def reset(self, key: chex.PRNGKey, params=None):
         obs, env_state = self._env.reset(key, params)
         state = LogEnvState(env_state, 0.0, 0, 0.0, 0, 0)
         return obs, state
 
-    @partial(jax.jit, static_argnums=(0, 4))
+    @partial(jax.jit, static_argnums=0)
     def step(
         self,
         key: chex.PRNGKey,
@@ -199,3 +265,4 @@ class LogWrapper(GymnaxWrapper):
         info["timestep"] = state.timestep
         info["returned_episode"] = done
         return obs, state, reward, done, info
+
