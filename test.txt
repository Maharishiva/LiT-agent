Directory tree for specified file types:
.
├── rel_multi_head.py
├── train_PPO_trXL.py
├── trainer_PPO_trXL.py
└── transformerXL.py

1 directory, 4 files

Contents of files:
---./trainer_PPO_trXL.py:
import jax
import jax.numpy as jnp
import flax.linen as nn
import numpy as np
import optax
from flax.linen.initializers import constant, orthogonal
from typing import Sequence, NamedTuple, Any
from flax.training.train_state import TrainState
import distrax
import gymnax
from gymnax.wrappers.purerl import  FlattenObservationWrapper
import wandb


# Custom TrainState to track step count
class CustomTrainState(TrainState):
    step_count: int


from wrappers import (
    LogWrapper,
    OptimisticResetVecEnvWrapper,
    AutoResetEnvWrapper,
    BatchEnvWrapper,
    ThinkingWrapper,
)


from transformerXL import Transformer

class ActorCriticTransformer(nn.Module):
    # action_dim: int
    action_dim_env: int
    thinking_vocab: int
    activation: str
    hidden_layers:int
    encoder_size: int
    num_heads: int
    qkv_features: int
    num_layers:int
    gating:bool=False
    gating_bias:float=0.
    
    
    def setup(self):
        self.action_dim = self.action_dim_env + self.thinking_vocab
        
        # USE SETUP AND DIFFERENT FUNCTIONS BECAUSE THE TRAIN IS DIFFERENT FROM EVAL ( as we query just one step in train and don't cache memory in eval)
        
        if self.activation == "relu":
            self.activation_fn = nn.relu
        else:
            self.activation_fn = nn.tanh
            
        self.transformer = Transformer(
                                encoder_size=self.encoder_size,
                                num_heads=self.num_heads,
                                qkv_features=self.qkv_features,
                                num_layers=self.num_layers,
                                gating=self.gating,
                                gating_bias=self.gating_bias,
                                env_action_dim=self.action_dim_env,
                                thinking_vocab=self.thinking_vocab)
        
        self.actor_ln1=nn.Dense(self.hidden_layers, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0))
        self.actor_ln2= nn.Dense(
            self.hidden_layers, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)
        )
        self.actor_out= nn.Dense(
            self.action_dim, kernel_init=orthogonal(0.01), bias_init=constant(0.0)
        )
        
        
        self.critic_ln1=nn.Dense(
            self.hidden_layers, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)
        )
        self.critic_ln2=nn.Dense(
            self.hidden_layers, kernel_init=orthogonal(np.sqrt(2)), bias_init=constant(0.0)
        )
        self.critic_out=nn.Dense(1, kernel_init=orthogonal(1.0), bias_init=constant(0.0))
        
        
        
        

    def __call__(self, memories, obs, prev_action=None, prev_reward=None, mask=None):
        x, memory_out = self.transformer(memories, obs, mask, prev_action, prev_reward)
    
        actor_mean = self.actor_ln1(x)
        actor_mean = self.activation_fn(actor_mean)
        actor_mean = self.actor_ln2(actor_mean)
        actor_mean = self.activation_fn(actor_mean)
        actor_mean = self.actor_out(actor_mean)
        pi = distrax.Categorical(logits=actor_mean)

        critic = self.critic_ln1(x)
        critic = self.activation_fn(critic)
        critic = self.critic_ln2(critic)
        critic = self.activation_fn(critic)
        critic = self.critic_out(
            critic
        )

        return pi, jnp.squeeze(critic, axis=-1), memory_out
    
    def model_forward_eval(self, memories, obs, prev_action=None, prev_reward=None, mask=None):
        """Used during environment rollout (single timestep of obs). And return the memory"""
        x, memory_out = self.transformer.forward_eval(memories, obs, mask, prev_action, prev_reward)

        actor_mean = self.actor_ln1(x)
        actor_mean = self.activation_fn(actor_mean)
        actor_mean = self.actor_ln2(actor_mean)
        actor_mean = self.activation_fn(actor_mean)
        actor_mean = self.actor_out(actor_mean)
        pi = distrax.Categorical(logits=actor_mean)

        critic = self.critic_ln1(x)
        critic = self.activation_fn(critic)
        critic = self.critic_ln2(critic)
        critic = self.activation_fn(critic)
        critic = self.critic_out(
            critic
        )

        return pi, jnp.squeeze(critic, axis=-1), memory_out
    
    def model_forward_train(self, memories, obs, prev_action=None, prev_reward=None, mask=None): 
        """Used during training: a window of observation is sent. And don't return the memory"""
        x = self.transformer.forward_train(memories, obs, mask, prev_action, prev_reward)

        actor_mean = self.actor_ln1(x)
        actor_mean = self.activation_fn(actor_mean)
        actor_mean = self.actor_ln2(actor_mean)
        actor_mean = self.activation_fn(actor_mean)
        actor_mean = self.actor_out(actor_mean)
        pi = distrax.Categorical(logits=actor_mean)

        critic = self.critic_ln1(x)
        critic = self.activation_fn(critic)
        critic = self.critic_ln2(critic)
        critic = self.activation_fn(critic)
        critic = self.critic_out(
            critic
        )
        return pi, jnp.squeeze(critic, axis=-1)
    


class Transition(NamedTuple):
    done: jnp.ndarray
    action: jnp.ndarray
    value: jnp.ndarray
    reward: jnp.ndarray
    log_prob: jnp.ndarray
    memories_mask: jnp.ndarray
    memories_indices: jnp.ndarray
    obs: jnp.ndarray
    info: jnp.ndarray
    prev_action: jnp.ndarray = None
    prev_reward: jnp.ndarray = None


    
indices_select=lambda x,y:x[y]
batch_indices_select=jax.vmap(indices_select)
roll_vmap=jax.vmap(jnp.roll,in_axes=(-2,0,None),out_axes=-2)
batchify=lambda x: jnp.reshape(x,(x.shape[0]*x.shape[1],)+x.shape[2:])


    
def make_train(config):
    
    config["NUM_UPDATES"] = (
        config["TOTAL_TIMESTEPS"] // config["NUM_STEPS"] // config["NUM_ENVS"]
    )
    config["MINIBATCH_SIZE"] = (
        config["NUM_ENVS"] * config["NUM_STEPS"] // config["NUM_MINIBATCHES"]
    )
    
    if(config["ENV_NAME"]=="craftax"):
        from craftax.craftax.envs.craftax_symbolic_env import CraftaxSymbolicEnvNoAutoReset
        env=CraftaxSymbolicEnvNoAutoReset()
        env_params=env.default_params
        action_dim_env = env.action_space(env_params).n
        env = LogWrapper(env)
        env = ThinkingWrapper(env, action_dim_env, config["R_THINK"])
        env = OptimisticResetVecEnvWrapper(
                env,
                num_envs=config["NUM_ENVS"],
                reset_ratio=min(16, config["NUM_ENVS"]),
            )
    else:
        env, env_params = gymnax.make(config["ENV_NAME"])
        action_dim_env = env.action_space(env_params).n 
        env = FlattenObservationWrapper(env)
        env = LogWrapper(env)
        env = ThinkingWrapper(env, action_dim_env, config["R_THINK"])
        env = BatchEnvWrapper(env,config["NUM_ENVS"])

    def linear_schedule(count):
        frac = 1.0 - (count // (config["NUM_MINIBATCHES"] * config["UPDATE_EPOCHS"])) / (config["NUM_UPDATES"]) 
        return config["LR"] * frac

    
    def train(rng):

        # INIT NETWORK
        network=ActorCriticTransformer(action_dim_env=env.action_space(env_params).n,
                             thinking_vocab=config["THINKING_VOCAB"],
                             activation=config["ACTIVATION"],
                            encoder_size=config["EMBED_SIZE"],
                            hidden_layers=config["hidden_layers"],
                            num_heads=config["num_heads"],
                            qkv_features=config["qkv_features"],
                            num_layers=config["num_layers"],
                            gating=config["gating"],
                            gating_bias=config["gating_bias"],)
        rng, _rng = jax.random.split(rng)
        init_obs = jnp.zeros((2,env.observation_space(env_params).shape[0]))
        init_memory = jnp.zeros((2,config["WINDOW_MEM"],config["num_layers"],config["EMBED_SIZE"]))
        init_mask = jnp.zeros((2,config["num_heads"],1,config["WINDOW_MEM"]+1),dtype=jnp.bool_)
        init_prev_action = jnp.zeros((2,), dtype=jnp.int32)  # Initial previous actions (zeros)
        init_prev_reward = jnp.zeros((2,))  # Initial previous rewards (zeros)
        
        network_params = network.init(_rng, init_memory, init_obs, init_prev_action, init_prev_reward, init_mask)
        
        
        
        
        if config["ANNEAL_LR"]:
            tx = optax.chain(
                optax.clip_by_global_norm(config["MAX_GRAD_NORM"]),
                optax.adam(learning_rate=linear_schedule, eps=1e-5),
            )
        else:
            tx = optax.chain(optax.clip_by_global_norm(config["MAX_GRAD_NORM"]), optax.adam(config["LR"], eps=1e-5))
            
            
        # Use CustomTrainState instead of TrainState to track step_count
        train_state = CustomTrainState.create(
            apply_fn=network.apply,
            params=network_params,
            tx=tx,
            step_count=0,
        )
        
        

        # Reset ENV
        rng, _rng = jax.random.split(rng)
        obsv, env_state = env.reset(_rng, None)
        #reset_rng = jax.random.split(_rng, config["NUM_ENVS"])
        #obsv, env_state = jax.vmap(env.reset, in_axes=(0, None))(reset_rng, env_params)
        

        # TRAIN LOOP
        def _update_step(runner_state, unused):
            
            # COLLECT TRAJECTORIES
            def _env_step(runner_state, unused):
                train_state, env_state, memories, memories_mask, memories_mask_idx, last_obs, done, step_env_currentloop, rng, last_action, last_reward = runner_state
                
                # reset memories mask and mask idx in cask of done otherwise mask will consider one more stepif not filled (if filled= 
                memories_mask_idx = jnp.where(done, config["WINDOW_MEM"], jnp.clip(memories_mask_idx-1, 0, config["WINDOW_MEM"]))
                memories_mask = jnp.where(done[:,None,None,None], jnp.zeros((config["NUM_ENVS"], config["num_heads"], 1, config["WINDOW_MEM"]+1), dtype=jnp.bool_), memories_mask)
                
                #Update memories mask with the potential additional step taken into account at this step
                memories_mask_idx_ohot = jax.nn.one_hot(memories_mask_idx, config["WINDOW_MEM"]+1)
                memories_mask_idx_ohot = memories_mask_idx_ohot[:,None,None,:].repeat(config["num_heads"], 1)
                memories_mask = jnp.logical_or(memories_mask, memories_mask_idx_ohot)
            
                # SELECT ACTION
                rng, _rng = jax.random.split(rng)
                
                # Use previous action and reward passed in runner state
                # For first step in episode (after reset), these will be zeros
                
                pi, value, memories_out = network.apply(
                    train_state.params, 
                    memories, 
                    last_obs,
                    last_action,
                    last_reward,
                    memories_mask,
                    method=network.model_forward_eval
                )
                ### mask the logits to prevent thinking if the thinking length exceeds the allowed limit ###
                logits = pi.logits
                thinking_length = env_state.thinking_length
                total_actions = logits.shape[-1]
                action_indices = jnp.arange(total_actions)
                allowed_mask = jnp.logical_or(
                    thinking_length[:, None] < config["MAX_THINKING_LEN"], # thinking len < max thinking len
                    action_indices[None, :] < network.action_dim_env, # action is env action
                )
                masked_logits = jnp.where(allowed_mask, logits, -jnp.inf)
                pi = distrax.Categorical(logits=masked_logits)
                ######
                
                action = pi.sample(seed=_rng)
                log_prob = pi.log_prob(action)
                
                # ADD THE CACHED ACTIVATIONS IN MEMORIES FOR NEXT STEP
                memories = jnp.roll(memories, -1, axis=1).at[:, -1].set(memories_out)

                # STEP ENV
                rng, _rng = jax.random.split(rng)
                obsv, env_state, reward, done, info = env.step(
                    _rng, env_state, action, None
                )
                
                #COMPUTE THE INDICES OF THE FINAL MEMORIES THAT ARE TAKEN INTO ACCOUNT IN THIS STEP 
                # not forgetting that we will concatenate the previous WINDOW_MEM to the NUM_STEPS so that even the first step will use some cached memory.
                #previous without this is attend to 0 which are masked but with reset happening if we start the num_steps loop during good to keep memory from previous
                memory_indices = jnp.arange(0, config["WINDOW_MEM"])[None, :] + step_env_currentloop * jnp.ones((config["NUM_ENVS"], 1), dtype=jnp.int32)
                
                transition = Transition(
                    done, action, value, reward, log_prob, memories_mask.squeeze(), memory_indices, 
                    last_obs, info, last_action, last_reward
                )
                
                # Update runner state with new action and reward for next step
                runner_state = (train_state, env_state, memories, memories_mask, memories_mask_idx, obsv, done, 
                               step_env_currentloop+1, rng, action, reward)
                
                return runner_state, (transition, memories_out)
            

            
            #also copy the first memories in memories_previous before the new rollout to concatenate previous memories with new steps so that first steps of new have memories
            memories_previous=runner_state[2]
             
            #SCAN THE STEP TO GET THE TRANSITIONS AND CACHED MEMORIES
            runner_state, (traj_batch,memories_batch) = jax.lax.scan(
                _env_step, runner_state, None, config["NUM_STEPS"]
            )

            # CALCULATE ADVANTAGE
            train_state, env_state, memories, memories_mask, memories_mask_idx, last_obs, done, _, rng, last_action, last_reward = runner_state
            
            # Use last action and reward from runner state
            _, last_val, _ = network.apply(
                train_state.params, 
                memories,
                last_obs,
                last_action,
                last_reward,
                memories_mask,
                method=network.model_forward_eval
            )

            def _calculate_gae(traj_batch, last_val):
                def _get_advantages(gae_and_next_value, transition):
                    gae, next_value = gae_and_next_value
                    done, value, reward, action = (
                        transition.done,
                        transition.value,
                        transition.reward,
                        transition.action
                    )
                    is_thinking = jnp.greater_equal(action, network.action_dim_env)
                    # reward = reward + jnp.where(is_thinking, config["R_THINK"], 0.0) # already accounted for in wrapper
                    gamma = jnp.where(is_thinking, 1.0, config["GAMMA"])
                    delta = reward + gamma * next_value * (1 - done) - value
                    gae = (
                        delta
                        + gamma * config["GAE_LAMBDA"] * (1 - done) * gae
                    )
                    return (gae, value), gae

                _, advantages = jax.lax.scan(
                    _get_advantages,
                    (jnp.zeros_like(last_val), last_val),
                    traj_batch,
                    reverse=True,
                    unroll=16,
                )
                return advantages, advantages + traj_batch.value

            advantages, targets = _calculate_gae(traj_batch, last_val)
            

            # UPDATE NETWORK
            def _update_epoch(update_state, unused):
                def _update_minbatch(train_state, batch_info):
                
                    traj_batch,memories_batch, advantages, targets = batch_info
                    def _loss_fn(params, traj_batch,memories_batch, gae, targets):
                        
                        
                        # USE THE CACHED MEMORIES ONLY FROM THE FIRST STEP OF A WINDOW GRAD Because all other will be computed again here.
                        #construct the memory batch from memory indices
                        memories_batch=batch_indices_select(memories_batch,traj_batch.memories_indices[:,::config["WINDOW_GRAD"]]) 
                        memories_batch=batchify(memories_batch)
                        
                        
                        #CREATE THE MASK FOR WINDOW GRAD (have to take the one from the batch and roll them to match the steps it attends
                        memories_mask=traj_batch.memories_mask.reshape((-1,config["WINDOW_GRAD"],)+traj_batch.memories_mask.shape[2:])
                        memories_mask=jnp.swapaxes(memories_mask,1,2)
                        #concatenate with 0s to fill before the roll
                        memories_mask=jnp.concatenate((memories_mask,jnp.zeros(memories_mask.shape[:-1]+(config["WINDOW_GRAD"]-1,),dtype=jnp.bool_)),axis=-1)
                        #roll of different value for each step to match the right
                        memories_mask=roll_vmap(memories_mask,jnp.arange(0,config["WINDOW_GRAD"]),-1)

                        #RESHAPE
                        obs=traj_batch.obs
                        obs=obs.reshape((-1,config["WINDOW_GRAD"] ,)+obs.shape[2:])

                        traj_batch,targets,gae=jax.tree_util.tree_map(lambda x : jnp.reshape(x,(-1,config["WINDOW_GRAD"])+x.shape[2:]),(traj_batch,targets,gae))
                      
  
                        
                        
                        # NETWORK OUTPUT
                        prev_action = traj_batch.prev_action
                        prev_reward = traj_batch.prev_reward
                        
                        pi, value = network.apply(
                            params,
                            memories_batch, 
                            obs,
                            prev_action,
                            prev_reward,
                            memories_mask,
                            method=network.model_forward_train
                        )
                        
                        log_prob = pi.log_prob(traj_batch.action)

                        # CALCULATE VALUE LOSS
                        value_pred_clipped = traj_batch.value + (
                            value - traj_batch.value
                        ).clip(-config["CLIP_EPS"], config["CLIP_EPS"])
                        value_losses = jnp.square(value - targets)
                        value_losses_clipped = jnp.square(value_pred_clipped - targets)
                        value_loss = (
                            0.5 * jnp.maximum(value_losses, value_losses_clipped).mean()
                        )

                        # CALCULATE ACTOR LOSS
                        ratio = jnp.exp(log_prob - traj_batch.log_prob)
                        gae = (gae - gae.mean()) / (gae.std() + 1e-8)
                        loss_actor1 = ratio * gae
                        loss_actor2 = (
                            jnp.clip(
                                ratio,
                                1.0 - config["CLIP_EPS"],
                                1.0 + config["CLIP_EPS"],
                            )
                            * gae
                        )
                        loss_actor = -jnp.minimum(loss_actor1, loss_actor2)
                        loss_actor = loss_actor.mean()
                        entropy = pi.entropy().mean()

                        total_loss = (
                            loss_actor
                            + config["VF_COEF"] * value_loss
                            - config["ENT_COEF"] * entropy
                        )
                        return total_loss, (value_loss, loss_actor, entropy)

                    grad_fn = jax.value_and_grad(_loss_fn, has_aux=True)
                    total_loss, grads = grad_fn(
                        train_state.params, traj_batch,memories_batch, advantages, targets
                    )
                    train_state = train_state.apply_gradients(grads=grads)
                    return train_state, total_loss

                train_state, traj_batch,memories_batch, advantages, targets, rng = update_state
                rng, _rng = jax.random.split(rng)            
                #batch_size = config["MINIBATCH_SIZE"] * config["NUM_MINIBATCHES"]
                assert (
                     config["NUM_STEPS"] % config["WINDOW_GRAD"]==0
                ), "NUM_STEPS should be divi by WINDOW_GRAD to properly batch the window_grad"
                
                
                # PERMUTE ALONG THE NUM_ENVS ONLY NOT TO LOOSE TRACK FROM TEMPORAL 
                permutation = jax.random.permutation(_rng, config["NUM_ENVS"])
                batch = (traj_batch,memories_batch, advantages, targets)
                batch = jax.tree_util.tree_map(
                    lambda x:  jnp.swapaxes(x,0,1),
                    batch,
                )
                shuffled_batch = jax.tree_util.tree_map(
                    lambda x: jnp.take(x, permutation, axis=0), batch
                )
                
                #either create memory batch here but might be big  or send all the memeory to loss and do the things with the index in the loss
                minibatches = jax.tree_util.tree_map(
                    lambda x: jnp.reshape(
                        x, [config["NUM_MINIBATCHES"], -1] + list(x.shape[1:])
                    ),
                    shuffled_batch,
                )
            
                train_state, total_loss = jax.lax.scan(
                    _update_minbatch, train_state, minibatches
                )

                update_state = (train_state, traj_batch,memories_batch, advantages, targets, rng)
                return update_state, total_loss
            
            
            #ADD PREVIOUS WINDOW_MEM To the current NUM_STEPS SO THAT FIRST STEPS USE MEMORIES FROM PREVIOUS
            # might be a better place to add the previous memory to the traj batch to make it faster ??? 
            #or another solution is to not add it but in training means that the first element might not look at info
            memories_batch=jnp.concatenate([jnp.swapaxes(memories_previous,0,1),memories_batch],axis=0)

            
            #CRAFTAX ONLY
            metric = jax.tree_map(
                lambda x: (x * traj_batch.info["returned_episode"]).sum()
                / traj_batch.info["returned_episode"].sum(),
                traj_batch.info,
            )
            metric=jax.tree_map(lambda x: x.mean(),metric)
            
            # Simplified wandb logging to match DQN pattern
            # Calculate timesteps - based on update count
            timesteps = runner_state[0].step_count * config["NUM_ENVS"] * config["NUM_STEPS"]
            
            # Create simple metrics dictionary with numeric values
            metrics = {
                "timesteps": timesteps,
                "update": runner_state[0].step_count,
                "return": metric["returned_episode_returns"],
                "episode_length": metric["returned_episode_lengths"],
                "thinking_count": metric["thinking_count"],
            }
            
            # Run the update epochs
            update_state = (train_state, traj_batch,memories_batch, advantages, targets, rng)
            update_state, loss_info = jax.lax.scan(
                _update_epoch, update_state, None, config["UPDATE_EPOCHS"]
            )
            
            train_state = update_state[0]
            
            # Increment step count
            train_state = train_state.replace(step_count=train_state.step_count + 1)
            
            # Log to wandb if enabled
            if config.get("WANDB_MODE", "disabled") == "online":
                def callback(update, return_val, episode_length, timesteps, thinking_count):
                    # Log every WANDB_LOG_FREQ updates
                    if update % config["WANDB_LOG_FREQ"] == 0:
                        wandb.log({
                            "return": float(return_val),
                            "episode_length": float(episode_length),
                            "timesteps": int(timesteps),
                            "update": int(update),
                            "thinking_count": float(thinking_count),
                        })
                
                jax.debug.callback(callback, metrics["update"], metrics["return"], 
                                  metrics["episode_length"], metrics["timesteps"], metrics["thinking_count"])
            
            rng = update_state[-1]
            # Reset step_env_currentloop to 0, but keep last_action and last_reward for the next batch
            runner_state = (train_state, env_state, memories, memories_mask, memories_mask_idx, 
                           last_obs, done, 0, rng, last_action, last_reward)
            return runner_state, metric
        
        
        # INITIALIZE the memories and memories mask 
        rng, _rng = jax.random.split(rng)
        memories = jnp.zeros((config["NUM_ENVS"], config["WINDOW_MEM"], config["num_layers"], config["EMBED_SIZE"]))
        memories_mask = jnp.zeros((config["NUM_ENVS"], config["num_heads"], 1, config["WINDOW_MEM"]+1), dtype=jnp.bool_)
        # memories +1 bc will remove one 
        memories_mask_idx = jnp.zeros((config["NUM_ENVS"],), dtype=jnp.int32) + (config["WINDOW_MEM"]+1)
        done = jnp.zeros((config["NUM_ENVS"],), dtype=jnp.bool_)
        
        # Initialize previous actions and rewards with zeros
        init_action = jnp.zeros((config["NUM_ENVS"],), dtype=jnp.int32)
        init_reward = jnp.zeros((config["NUM_ENVS"],))
        
        runner_state = (train_state, env_state, memories, memories_mask, memories_mask_idx, 
                        obsv, done, 0, _rng, init_action, init_reward)
        runner_state, metric = jax.lax.scan(
            _update_step, runner_state, None, config["NUM_UPDATES"]
        )
        return {"runner_state": runner_state, "metrics": metric}

    return train
---./train_PPO_trXL.py:
import os
import time
from trainer_PPO_trXL import make_train
import wandb
import jax
import jax.numpy as jnp

# Set device in a safer way
try:
    # Try to print available devices to help with debugging
    print("Available devices:", jax.devices())
except:
    # If that fails, just set CPU platform directly
    print("Error listing devices, forcing CPU")
    os.environ["JAX_PLATFORMS"] = "cpu"

config = {
    "LR": 2e-4,
    "NUM_ENVS": 1024,
    "NUM_STEPS": 128,
    "TOTAL_TIMESTEPS": 1e9,
    "UPDATE_EPOCHS": 4,
    "NUM_MINIBATCHES": 8,
    "GAMMA": 0.999,
    "GAE_LAMBDA": 0.9,
    "CLIP_EPS": 0.2,
    "ENT_COEF": 0.03,
    "VF_COEF": 0.5,
    "MAX_GRAD_NORM": 1.,
    "ACTIVATION": "relu",
    "ENV_NAME": "craftax",
    "ANNEAL_LR": True,
    "qkv_features": 256,
    "EMBED_SIZE": 256,
    "num_heads": 8,
    "num_layers": 3,
    "hidden_layers": 256,
    "WINDOW_MEM": 256,
    "WINDOW_GRAD": 128,
    "gating": True,
    "gating_bias": 2.,
    "seed": 0,
    "WANDB_MODE": "online",  # Set to "online" to enable wandb logging
    "WANDB_PROJECT": "lit-transformer-ppo",
    "WANDB_ENTITY": "maharishiva",  # Set to your wandb username or team name
    "WANDB_LOG_FREQ": 1,    # Log every N updates
    "THINKING_VOCAB": 64,
    "R_THINK": 0.0,
    "MAX_THINKING_LEN": 8,
}

# Initialize wandb if enabled
if config["WANDB_MODE"] == "online":
    wandb.init(
        project=config["WANDB_PROJECT"],
        entity=config["WANDB_ENTITY"],
        config=config,
        name=f"{config['ENV_NAME']}_seed{config['seed']}",
    )

seed=config["seed"]

prefix = "results_craftax/"+config["ENV_NAME"]

# Create directories with proper error handling
try:
    if not os.path.exists("results_craftax"):
        os.makedirs("results_craftax")
    if not os.path.exists(prefix):
        os.makedirs(prefix)
    print(f"Saving results to {prefix}")
except Exception as e:
    print(f"Directory creation {prefix} failed: {str(e)}")
    
print("Start compiling and training")
time_a=time.time()
rng = jax.random.PRNGKey(seed)

train_jit = jax.jit(make_train(config))
print(f"Compilation finished in {time.time() - time_a:.2f} seconds")

out = train_jit(rng)
print("training and compilation took " + str(time.time()-time_a))

# Close wandb run if it was enabled
if config["WANDB_MODE"] == "online":
    wandb.finish()

import matplotlib.pyplot as plt
plt.plot(out["metrics"]["returned_episode_returns"])
plt.xlabel("Updates")
plt.ylabel("Return")
plt.savefig(prefix+"/return_"+str(seed))

plt.clf()

jnp.save(prefix+"/"+str(seed)+"_params", out["runner_state"][0].params)
jnp.save(prefix+"/"+str(seed)+"_config", config)

jnp.save(prefix+"/"+str(seed)+"_metrics",out["metrics"])

---./rel_multi_head.py:
# Copyright 2023 The Flax Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#         http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.



# CODE IS HEAVILY INSPIRED FROM https://github.com/huggingface/transformers/blob/v4.40.1/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py
# MOST OF THE TIME JUST A CONVERSION IN JAX


"""Relative Attention HEAVILY INSPIRED FROM https://github.com/huggingface/transformers/blob/v4.40.1/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py
, flax attention, https://github.com/kimiyoung/transformer-xl/blob/master/pytorch/mem_transformer.py#L143, most of the time just a flax/jax conversion """

import functools
from typing import (Any, Callable, Optional, Tuple)
from flax.linen.dtypes import promote_dtype

from flax.linen import initializers
from flax.linen.linear import default_kernel_init
from flax.linen.linear import DenseGeneral
from flax.linen.linear import DotGeneralT
from flax.linen.linear import PrecisionLike
from flax.linen.module import compact
from flax.linen.module import merge_param
from flax.linen.module import Module

import jax
from jax import lax
from jax import random
import jax.numpy as jnp

PRNGKey = Any
Shape = Tuple[int, ...]
Dtype = Any
Array = Any

roll_vmap=jax.vmap(jnp.roll,in_axes=(-2,0,None),out_axes=-2)



def _rel_shift( x):
        zero_pad_shape =   x.shape[:-2]+(x.shape[-2], 1)
        zero_pad = jnp.zeros(zero_pad_shape, dtype=x.dtype)
        x_padded = jnp.concatenate([zero_pad, x], axis=-1)

        x_padded_shape =   x.shape[:-2] + (x.shape[-1] + 1, x.shape[-2])
        x_padded = x_padded.reshape(x_padded_shape)
        #x_padded=jnp.swapaxes(x_padded,0,1)

        x = jnp.take(x_padded,jnp.arange(1,x_padded.shape[-2]),axis=-2).reshape(x.shape)

        return x

def dot_product_attention_weights(
        query: Array,
        key: Array,
        r_pos_embed,
        r_r_bias,
        r_w_bias,
        bias: Optional[Array] = None,
        mask: Optional[Array] = None,
        broadcast_dropout: bool = True,
        dropout_rng: Optional[PRNGKey] = None,
        dropout_rate: float = 0.0,
        deterministic: bool = False,
        dtype: Optional[Dtype] = None,
        precision: PrecisionLike = None,
):
    """Computes dot-product attention weights given query and key.

    Used by :func:`dot_product_attention`, which is what you'll most likely use.
    But if you want access to the attention weights for introspection, then
    you can directly call this function and call einsum yourself.

    Args:
        query: queries for calculating attention with shape of
            `[batch..., q_length, num_heads, qk_depth_per_head]`.
        key: keys for calculating attention with shape of
            `[batch..., kv_length, num_heads, qk_depth_per_head]`.
        bias: bias for the attention weights. This should be broadcastable to the
            shape `[batch..., num_heads, q_length, kv_length]`.
            This can be used for incorporating causal masks, padding masks,
            proximity bias, etc.
        mask: mask for the attention weights. This should be broadcastable to the
            shape `[batch..., num_heads, q_length, kv_length]`.
            This can be used for incorporating causal masks.
            Attention weights are masked out if their corresponding mask value
            is `False`.
        broadcast_dropout: bool: use a broadcasted dropout along batch dims.
        dropout_rng: JAX PRNGKey: to be used for dropout
        dropout_rate: dropout rate
        deterministic: bool, deterministic or not (to apply dropout)
        dtype: the dtype of the computation (default: infer from inputs and params)
        precision: numerical precision of the computation see `jax.lax.Precision`
            for details.

    Returns:
        Output of shape `[batch..., num_heads, q_length, kv_length]`.
    """
    query, key = promote_dtype(query, key, dtype=dtype)
    dtype = query.dtype

    assert query.ndim == key.ndim, 'q, k must have same rank.'
    assert query.shape[:-3] == key.shape[:-3], 'q, k batch dims must match.'
    assert query.shape[-2] == key.shape[-2], 'q, k num_heads must match.'
    assert query.shape[-1] == key.shape[-1], 'q, k depths must match.'

    # calculate attention matrix
    depth = query.shape[-1]
    #query = query 
    
    
    
    # attn weight shape is (batch..., num_heads, q_length, kv_length)
    attn_weights = jnp.einsum(
            '...qhd,...khd->...hqk', query+r_w_bias, key, precision=precision
    )
    
    attn_weights_r= jnp.einsum(
        '...qhd,khd->...hqk', query+r_r_bias, r_pos_embed, precision=precision
    )
    
    
    
    #TODO see which one is faster (should be doing the same thing too , mb just take rel_shift as same as huggingface impl)
    attn_weights_r=roll_vmap(attn_weights_r,jnp.arange(0,query.shape[-3])-(query.shape[-3]-1),-1)
    #attn_weights_r=_rel_shift(attn_weights_r)
    attn_weights=attn_weights+attn_weights_r
    
    
    attn_weights=attn_weights / jnp.sqrt(depth).astype(dtype)
    
    # apply attention bias: masking, dropout, proximity bias, etc.
    if bias is not None:
        attn_weights = attn_weights + bias
    # apply attention mask
    if mask is not None:
        big_neg = jnp.finfo(dtype).min
        attn_weights = jnp.where(mask, attn_weights, big_neg)

    # normalize the attention weights
    attn_weights = jax.nn.softmax(attn_weights).astype(dtype)

    # apply attention dropout
    if not deterministic and dropout_rate > 0.0:
        keep_prob = 1.0 - dropout_rate
        if broadcast_dropout:
            # dropout is broadcast across the batch + head dimensions
            dropout_shape = tuple([1] * (key.ndim - 2)) + attn_weights.shape[-2:]
            keep = random.bernoulli(dropout_rng, keep_prob, dropout_shape)    # type: ignore
        else:
            keep = random.bernoulli(dropout_rng, keep_prob, attn_weights.shape)    # type: ignore
        multiplier = keep.astype(dtype) / jnp.asarray(keep_prob, dtype=dtype)
        attn_weights = attn_weights * multiplier

    return attn_weights


def dot_product_attention(
        query: Array,
        key: Array,
        value: Array,
        r_pos_embed,
        r_r_bias,
        r_w_bias,
        bias: Optional[Array] = None,
        mask: Optional[Array] = None,
        broadcast_dropout: bool = True,
        dropout_rng: Optional[PRNGKey] = None,
        dropout_rate: float = 0.0,
        deterministic: bool = False,
        dtype: Optional[Dtype] = None,
        precision: PrecisionLike = None,
):
    """Computes dot-product attention given query, key, and value.

    This is the core function for applying attention based on
    https://arxiv.org/abs/1706.03762. It calculates the attention weights given
    query and key and combines the values using the attention weights.

    Note: query, key, value needn't have any batch dimensions.

    Args:
        query: queries for calculating attention with shape of
            `[batch..., q_length, num_heads, qk_depth_per_head]`.
        key: keys for calculating attention with shape of
            `[batch..., kv_length, num_heads, qk_depth_per_head]`.
        value: values to be used in attention with shape of
            `[batch..., kv_length, num_heads, v_depth_per_head]`.
        bias: bias for the attention weights. This should be broadcastable to the
            shape `[batch..., num_heads, q_length, kv_length]`.
            This can be used for incorporating causal masks, padding masks,
            proximity bias, etc.
        mask: mask for the attention weights. This should be broadcastable to the
            shape `[batch..., num_heads, q_length, kv_length]`.
            This can be used for incorporating causal masks.
            Attention weights are masked out if their corresponding mask value
            is `False`.
        broadcast_dropout: bool: use a broadcasted dropout along batch dims.
        dropout_rng: JAX PRNGKey: to be used for dropout
        dropout_rate: dropout rate
        deterministic: bool, deterministic or not (to apply dropout)
        dtype: the dtype of the computation (default: infer from inputs)
        precision: numerical precision of the computation see `jax.lax.Precision`
            for details.

    Returns:
        Output of shape `[batch..., q_length, num_heads, v_depth_per_head]`.
    """
    query, key, value = promote_dtype(query, key, value, dtype=dtype)
    dtype = query.dtype
    assert key.ndim == query.ndim == value.ndim, 'q, k, v must have same rank.'
    assert (
            query.shape[:-3] == key.shape[:-3] == value.shape[:-3]
    ), 'q, k, v batch dims must match.'
    assert (
            query.shape[-2] == key.shape[-2] == value.shape[-2]
    ), 'q, k, v num_heads must match.'
    assert key.shape[-3] == value.shape[-3], 'k, v lengths must match.'

    # compute attention weights
    attn_weights = dot_product_attention_weights(
            query,
            key,
            r_pos_embed,
            r_r_bias,
            r_w_bias,
            bias,
            mask,
            broadcast_dropout,
            dropout_rng,
            dropout_rate,
            deterministic,
            dtype,
            precision,
    )

    # return weighted sum over values for each query position
    return jnp.einsum(
            '...hqk,...khd->...qhd', attn_weights, value, precision=precision
    )


class RelMultiHeadDotProductAttention(Module):
    """Multi-head dot-product attention.

    Attributes:
        num_heads: number of attention heads. Features (i.e. inputs_q.shape[-1])
            should be divisible by the number of heads.
        dtype: the dtype of the computation (default: infer from inputs and params)
        param_dtype: the dtype passed to parameter initializers (default: float32)
        qkv_features: dimension of the key, query, and value.
        out_features: dimension of the last projection
        broadcast_dropout: bool: use a broadcasted dropout along batch dims.
        dropout_rate: dropout rate
        deterministic: if false, the attention weight is masked randomly using
            dropout, whereas if true, the attention weights are deterministic.
        precision: numerical precision of the computation see `jax.lax.Precision`
            for details.
        kernel_init: initializer for the kernel of the Dense layers.
        bias_init: initializer for the bias of the Dense layers.
        use_bias: bool: whether pointwise QKVO dense transforms use bias.
        attention_fn: dot_product_attention or compatible function. Accepts query,
            key, value, and returns output of shape `[bs, dim1, dim2, ..., dimN,,
            num_heads, value_channels]``
        decode: whether to prepare and use an autoregressive cache.
    """

    num_heads: int
    dtype: Optional[Dtype] = None
    param_dtype: Dtype = jnp.float32
    qkv_features: Optional[int] = None
    out_features: Optional[int] = None
    broadcast_dropout: bool = True
    dropout_rate: float = 0.0
    deterministic: Optional[bool] = None
    precision: PrecisionLike = None
    kernel_init: Callable[[PRNGKey, Shape, Dtype], Array] = default_kernel_init
    bias_init: Callable[[PRNGKey, Shape, Dtype], Array] = (
            initializers.zeros_init()
    )
    use_bias: bool = True
    attention_fn: Callable[..., Array] = dot_product_attention
    decode: bool = False
    qkv_dot_general: DotGeneralT = lax.dot_general
    out_dot_general: DotGeneralT = lax.dot_general

    @compact
    def __call__(
            self,
            inputs_q: Array,
            inputs_kv: Array,
            pos_embed:Array,
            mask: Optional[Array] = None,
            deterministic: Optional[bool] = None,
    ):
        """Applies multi-head dot product attention on the input data.

        Projects the inputs into multi-headed query, key, and value vectors,
        applies dot-product attention and project the results to an output vector.

        Args:
            inputs_q: input queries of shape
                `[batch_sizes..., length, features]`.
            inputs_kv: key/values of shape
                `[batch_sizes..., length, features]`.
            mask: attention mask of shape
                `[batch_sizes..., num_heads, query_length, key/value_length]`.
                Attention weights are masked out if their corresponding mask value
                is `False`.
            deterministic: if false, the attention weight is masked randomly
                using dropout, whereas if true, the attention weights
                are deterministic.

        Returns:
            output of shape `[batch_sizes..., length, features]`.
        """
        features = self.out_features or inputs_q.shape[-1]
        qkv_features = self.qkv_features or inputs_q.shape[-1]
        assert qkv_features % self.num_heads == 0, (
                f'Memory dimension ({qkv_features}) must be divisible by number of'
                f' heads ({self.num_heads}).'
        )
        head_dim = qkv_features // self.num_heads

        dense = functools.partial(
                DenseGeneral,
                axis=-1,
                dtype=self.dtype,
                param_dtype=self.param_dtype,
                features=(self.num_heads, head_dim),
                kernel_init=self.kernel_init,
                bias_init=self.bias_init,
                use_bias=self.use_bias,
                precision=self.precision,
                dot_general=self.qkv_dot_general,
        )
        # project inputs_q to multi-headed q/k/v
        # dimensions are then [batch..., length, n_heads, n_features_per_head]
        query, key, value = (
                dense(name='query')(inputs_q),
                dense(name='key')(inputs_kv),
                dense(name='value')(inputs_kv),
        )
        
        #different bc no bias
        dense_relpos = functools.partial(
          DenseGeneral,
          axis=-1,
          dtype=self.dtype,
          param_dtype=self.param_dtype,
          features=(self.num_heads, head_dim),
          kernel_init=self.kernel_init,
          use_bias=False,
          precision=self.precision,
          dot_general=self.qkv_dot_general,
        )

        r_pos_embed =  dense_relpos(name='pos_embed_mat')(pos_embed)
        
        
        r_r_bias = self.param('r_r_bias',
                        self.bias_init, # Initialization function
                        (self.num_heads, head_dim)) 
        r_w_bias = self.param('r_w_bias',
                        self.bias_init, # Initialization function
                        (self.num_heads, head_dim))
         


        # During fast autoregressive decoding, we feed one position at a time,
        # and cache the keys and values step by step.
        if self.decode:
            # detect if we're initializing by absence of existing cache data.
            is_initialized = self.has_variable('cache', 'cached_key')
            cached_key = self.variable(
                    'cache', 'cached_key', jnp.zeros, key.shape, key.dtype
            )
            cached_value = self.variable(
                    'cache', 'cached_value', jnp.zeros, value.shape, value.dtype
            )
            cache_index = self.variable(
                    'cache', 'cache_index', lambda: jnp.array(0, dtype=jnp.int32)
            )
            if is_initialized:
                (
                        *batch_dims,
                        max_length,
                        num_heads,
                        depth_per_head,
                ) = cached_key.value.shape
                # shape check of cached keys against query input
                expected_shape = tuple(batch_dims) + (1, num_heads, depth_per_head)
                if expected_shape != query.shape:
                    raise ValueError(
                            'Autoregressive cache shape error, '
                            'expected query shape %s instead got %s.'
                            % (expected_shape, query.shape)
                    )
                # update key, value caches with our new 1d spatial slices
                cur_index = cache_index.value
                indices = (0,) * len(batch_dims) + (cur_index, 0, 0)
                key = lax.dynamic_update_slice(cached_key.value, key, indices)
                value = lax.dynamic_update_slice(cached_value.value, value, indices)
                cached_key.value = key
                cached_value.value = value
                cache_index.value = cache_index.value + 1
                # causal mask for cached decoder self-attention:
                # our single query position should only attend to those key
                # positions that have already been generated and cached,
                # not the remaining zero elements.
                mask = combine_masks(
                        mask,
                        jnp.broadcast_to(
                                jnp.arange(max_length) <= cur_index,
                                tuple(batch_dims) + (1, 1, max_length),
                        ),
                )

        dropout_rng = None
        if (
                self.dropout_rate > 0.0
        ):    # Require `deterministic` only if using dropout.
            m_deterministic = merge_param(
                    'deterministic', self.deterministic, deterministic
            )
            if not m_deterministic:
                dropout_rng = self.make_rng('dropout')
        else:
            m_deterministic = True

        # apply attention
        x = self.attention_fn(
                query,
                key,
                value,
                r_pos_embed,
                r_r_bias,
                r_w_bias,
                mask=mask,
                dropout_rng=dropout_rng,
                dropout_rate=self.dropout_rate,
                broadcast_dropout=self.broadcast_dropout,
                deterministic=m_deterministic,
                dtype=self.dtype,
                precision=self.precision,
        )    # pytype: disable=wrong-keyword-args
        # back to the original inputs dimensions
        out = DenseGeneral(
                features=features,
                axis=(-2, -1),
                kernel_init=self.kernel_init,
                bias_init=self.bias_init,
                use_bias=self.use_bias,
                dtype=self.dtype,
                param_dtype=self.param_dtype,
                precision=self.precision,
                dot_general=self.out_dot_general,
                name='out',    # type: ignore[call-arg]
        )(x)
        return out


class SelfAttention(RelMultiHeadDotProductAttention):
    """Self-attention special case of multi-head dot-product attention."""

    @compact
    def __call__(    # type: ignore
            self,
            inputs_q: Array,
            mask: Optional[Array] = None,
            deterministic: Optional[bool] = None,
    ):
        """Applies multi-head dot product self-attention on the input data.

        Projects the inputs into multi-headed query, key, and value vectors,
        applies dot-product attention and project the results to an output vector.

        Args:
            inputs_q: input queries of shape
                `[batch_sizes..., length, features]`.
            mask: attention mask of shape
                `[batch_sizes..., num_heads, query_length, key/value_length]`.
                Attention weights are masked out if their corresponding mask value
                is `False`.
            deterministic: if false, the attention weight is masked randomly
                using dropout, whereas if true, the attention weights
                are deterministic.

        Returns:
            output of shape `[batch_sizes..., length, features]`.
        """
        return super().__call__(
                inputs_q, inputs_q, mask, deterministic=deterministic
        )


# mask-making utility functions


def make_attention_mask(
        query_input: Array,
        key_input: Array,
        pairwise_fn: Callable[..., Any] = jnp.multiply,
        extra_batch_dims: int = 0,
        dtype: Dtype = jnp.float32,
):
    """Mask-making helper for attention weights.

    In case of 1d inputs (i.e., `[batch..., len_q]`, `[batch..., len_kv]`, the
    attention weights will be `[batch..., heads, len_q, len_kv]` and this
    function will produce `[batch..., 1, len_q, len_kv]`.

    Args:
        query_input: a batched, flat input of query_length size
        key_input: a batched, flat input of key_length size
        pairwise_fn: broadcasting elementwise comparison function
        extra_batch_dims: number of extra batch dims to add singleton
            axes for, none by default
        dtype: mask return dtype

    Returns:
        A `[batch..., 1, len_q, len_kv]` shaped mask for 1d attention.
    """
    mask = pairwise_fn(
            jnp.expand_dims(query_input, axis=-1), jnp.expand_dims(key_input, axis=-2)
    )
    mask = jnp.expand_dims(mask, axis=-3)
    mask = jnp.expand_dims(mask, axis=tuple(range(extra_batch_dims)))
    return mask.astype(dtype)


def make_causal_mask(
        x: Array, extra_batch_dims: int = 0, dtype: Dtype = jnp.float32
) -> Array:
    """Make a causal mask for self-attention.

    In case of 1d inputs (i.e., `[batch..., len]`, the self-attention weights
    will be `[batch..., heads, len, len]` and this function will produce a
    causal mask of shape `[batch..., 1, len, len]`.

    Args:
        x: input array of shape `[batch..., len]`
        extra_batch_dims: number of batch dims to add singleton axes for,
            none by default
        dtype: mask return dtype

    Returns:
        A `[batch..., 1, len, len]` shaped causal mask for 1d attention.
    """
    idxs = jnp.broadcast_to(jnp.arange(x.shape[-1], dtype=jnp.int32), x.shape)
    return make_attention_mask(
            idxs,
            idxs,
            jnp.greater_equal,
            extra_batch_dims=extra_batch_dims,
            dtype=dtype,
    )


def combine_masks(*masks: Optional[Array], dtype: Dtype = jnp.float32) -> Array:
    """Combine attention masks.

    Args:
        *masks: set of attention mask arguments to combine, some can be None.
        dtype: dtype for the returned mask.

    Returns:
        Combined mask, reduced by logical and, returns None if no masks given.
    """
    masks_list = [m for m in masks if m is not None]
    if not masks_list:
        return None
    assert all(
            map(lambda x: x.ndim == masks_list[0].ndim, masks_list)
    ), f'masks must have same rank: {tuple(map(lambda x: x.ndim, masks_list))}'
    mask, *other_masks = masks_list
    for other_mask in other_masks:
        mask = jnp.logical_and(mask, other_mask)
    return mask.astype(dtype)

---./transformerXL.py:
import jax
import jax.numpy as jnp
import flax.linen as nn
from flax.linen.initializers import constant, orthogonal

from rel_multi_head import RelMultiHeadDotProductAttention

# CODE IS HEAVILY INSPIRED FROM https://github.com/huggingface/transformers/blob/v4.40.1/src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py
# MOST OF THE TIME JUST A CONVERSION IN JAX
# AS WELL AS INSPIRATIONS FROM https://github.com/MarcoMeter/episodic-transformer-memory-ppo

class Gating(nn.Module):
    #code taken from https://github.com/dhruvramani/Transformers-RL/blob/master/layers.py
    d_input:int
    bg:float=0.
    @nn.compact
    def __call__(self,x,y):
        r = jax.nn.sigmoid(nn.Dense(self.d_input,use_bias=False)(y) + nn.Dense(self.d_input,use_bias=False)(x))
        z = jax.nn.sigmoid(nn.Dense(self.d_input,use_bias=False)(y) + nn.Dense(self.d_input,use_bias=False)(x) - self.param('gating_bias',constant(self.bg),(self.d_input,)))
        h = jnp.tanh(nn.Dense(self.d_input,use_bias=False)(y) + nn.Dense(self.d_input,use_bias=False)(r*x))
        g = (1 - z)* x + (z*  h)
        return g


class transformer_layer(nn.Module):
    num_heads: int
    out_features: int
    qkv_features: int
    gating:bool =False
    gating_bias:float =0.

    def setup(self):
        self.attention1 = RelMultiHeadDotProductAttention(num_heads=self.num_heads, qkv_features=self.qkv_features,
                                           out_features=self.out_features)

        self.ln1 = nn.LayerNorm()

        self.dense1 = nn.Dense(self.out_features)

        self.dense2 = nn.Dense(self.out_features)

        self.ln2 = nn.LayerNorm()
        if(self.gating):
            self.gate1=Gating(self.out_features,self.gating_bias)
            self.gate2=Gating(self.out_features,self.gating_bias)

        
        
    def __call__(self, values_keys:jnp.ndarray, queries:jnp.ndarray, pos_embed:jnp.ndarray, mask: jnp.ndarray):
        
        
        ### Post norm
        
        #out_attention = queries+ self.attention1(inputs_kv=keys,inputs_q=queries,mask=mask)
        #out_attention = self.ln1(out_attention)

        #out = self.dense1(out_attention)
        #out = nn.activation.relu(out)
        #out = self.dense2(out_attention)

        #out = out + out_attention

        #out = self.ln2(out)
        
        #pre norm
        values_keys=self.ln1(values_keys)
        queries_n=self.ln1(queries)
        attention= self.attention1(inputs_kv=values_keys,inputs_q=queries_n,mask=mask,pos_embed=pos_embed)
        if(self.gating):
            out_attention= self.gate1(queries,jax.nn.relu(attention))
        else:
            out_attention = queries+ attention

        out_attention_n=self.ln2(out_attention)
        out = self.dense1(out_attention_n)
        out = nn.activation.gelu(out)
        #out = nn.activation.relu(out)
        out = self.dense2(out)
        if(self.gating):
            out= self.gate2(out,jax.nn.relu(out_attention))
        else:
            out = out + out_attention


        return out





    

class PositionalEmbedding(nn.Module):
    dim_emb:int
    def setup(self):

        self.inv_freq = 1 / (10000 ** (jnp.arange(0.0, self.dim_emb, 2.0) / self.dim_emb))

    def __call__(self, pos_seq, bsz=None):
        sinusoid_inp = jnp.outer(pos_seq, self.inv_freq)
        pos_emb = jnp.concatenate([jnp.sin(sinusoid_inp), jnp.cos(sinusoid_inp)], axis=-1)

        #if bsz is not None:
        #    return pos_emb[:, None, :].expand(-1, bsz, -1)
        #else:
        #    return pos_emb[:, None, :]
        return pos_emb
                        
                        
class Transformer(nn.Module):
    encoder_size: int
    num_heads: int
    qkv_features: int
    num_layers: int
    env_action_dim: int  # number of real environment actions
    thinking_vocab: int  # number of thinking actions
    gating: bool = False
    gating_bias: float = 0.

    @property
    def total_action_dim(self):
        return self.env_action_dim + self.thinking_vocab

    def setup(self):
        self.encoder = nn.Dense(self.encoder_size)
        
        # separate embeddings for environment and thinking actions
        self.env_action_embed = nn.Embed(num_embeddings=self.env_action_dim, features=self.encoder_size)
        self.thinking_embed = nn.Embed(num_embeddings=self.thinking_vocab, features=self.encoder_size)
        self.reward_embed = nn.Dense(self.encoder_size)
        self.token_projection = nn.Dense(self.encoder_size)
        
        self.tf_layers = [transformer_layer(num_heads=self.num_heads, qkv_features=self.qkv_features,
                                           out_features=self.encoder_size,
                                           gating=self.gating, gating_bias=self.gating_bias) for _ in range(self.num_layers)]
        
        self.pos_emb = PositionalEmbedding(self.encoder_size)

    def _create_combined_token(self, obs, prev_action, prev_reward):
        """Create a token from observation and (prev_action, prev_reward).

        If prev_action corresponds to a thinking action, the token is *only* the
        thinking-action embedding.  Otherwise the token is the projected
        concatenation of [state_emb, action_emb, reward_emb].
        """
        state_emb = self.encoder(obs)

        # Determine whether the previous action was a thinking action.
        is_think = jnp.greater_equal(prev_action, self.env_action_dim)  # bool array

        # Compute indices for embedding look-ups (keep them in-range regardless of branch)
        env_idx = jnp.where(is_think, 0, prev_action)
        think_idx = jnp.where(is_think, prev_action - self.env_action_dim, 0)

        env_action_emb = self.env_action_embed(env_idx)
        thinking_emb = self.thinking_embed(think_idx)

        # Embed reward (always used in non-thinking branch)
        reward_emb = self.reward_embed(prev_reward[..., None] if state_emb.ndim > prev_reward.ndim else prev_reward)

        # Build env token: concat then linear projection
        env_token = self.token_projection(jnp.concatenate([state_emb, env_action_emb, reward_emb], axis=-1))

        # Select between thinking token and env token
        token = jnp.where(is_think[..., None], thinking_emb, env_token)
        return token
    
    def __call__(self, memories, obs, mask, prev_action=None, prev_reward=None):
        # Create token from obs, prev_action, and prev_reward
        x = self._create_combined_token(obs, prev_action, prev_reward)
        pos_embed = self.pos_emb(jnp.arange(1+memories.shape[-3],-1,-1))[:1+memories.shape[-3]]

        i = 0
        for layer in self.tf_layers:
            memory = jnp.concatenate([memories[:,:,i], x[:,None]], axis=-2)
            x = layer(values_keys=memory, queries=x[:,None], pos_embed=pos_embed, mask=mask)
            x = x.squeeze()
            i = i+1
            
        return x

    def forward_eval(self, memories, obs, mask, prev_action=None, prev_reward=None):
        # Create token from obs, prev_action, and prev_reward
        x = self._create_combined_token(obs, prev_action, prev_reward)
        
        out_memory = jnp.zeros((x.shape[0], self.num_layers) + x.shape[1:])
        i = 0
        
        pos_embed = self.pos_emb(jnp.arange(1+memories.shape[-3],-1,-1))[:1+memories.shape[-3]]      
        
        for layer in self.tf_layers:
            out_memory = out_memory.at[:,i].set(x)
            
            memory = jnp.concatenate([memories[:,:,i], x[:,None]], axis=-2)
            x = layer(values_keys=memory, pos_embed=pos_embed, queries=x[:,None], mask=mask)
            x = x.squeeze()
            i = i+1
            
        return x, out_memory
    
    def forward_train(self, memories, obs, mask, prev_action=None, prev_reward=None):
        # Create token from obs, prev_action, and prev_reward
        x = self._create_combined_token(obs, prev_action, prev_reward)
        
        pos_embed = self.pos_emb(jnp.arange(x.shape[-2]+memories.shape[-3],-1,-1))[:x.shape[-2]+memories.shape[-3]]
        
        i = 0
        for layer in self.tf_layers:
            memory = jnp.concatenate([jnp.take(memories, i, -2), x], axis=-2)
            x = layer(values_keys=memory, pos_embed=pos_embed, queries=x, mask=mask)
            i = i+1

        return x
    
    
    

